{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-processing\n",
    "\n",
    "**Tasks**\n",
    "- remove html tags "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# text pre-processing\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# text modelling \n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import spacy\n",
    "\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\" \n",
    "    Produces the text with html tags removed and converts to all lower case. \n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    text (pandas.core.series.Series) A series of text documents. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.core.series.Series A series of text with html tags removed & lower case letters. \n",
    "        \n",
    "    \"\"\"\n",
    "    # initialize a list for cleaned text \n",
    "    clean_text = []\n",
    "    for doc in text:\n",
    "        \n",
    "        ## remove html tags with beautifulsoup \n",
    "        soup = BeautifulSoup(doc)\n",
    "        text = soup.get_text().lower()\n",
    "\n",
    "        # append the text to a new series \n",
    "        clean_text.append(text)\n",
    "\n",
    "    # convert list to a pandas series \n",
    "    clean_text = pd.Series(clean_text)\n",
    "    \n",
    "    return clean_text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/Train.csv\")\n",
    "X_train = train.text\n",
    "y_train = train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I grew up (b. 1965) watching and loving the Th...\n",
       "1    When I put this movie in my DVD player, and sa...\n",
       "2    Why do people who do not know what a particula...\n",
       "3    Even though I have great interest in Biblical ...\n",
       "4    Im a die hard Dads Army fan and nothing will e...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-process text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process the text \n",
    "X_train = preprocess_text(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i grew up (b. 1965) watching and loving the thunderbirds. all my mates at school watched. we played \"thunderbirds\" before school, during lunch and after school. we all wanted to be virgil or scott. no one wanted to be alan. counting down from 5 became an art form. i took my children to see the movie hoping they would get a glimpse of what i loved as a child. how bitterly disappointing. the only high point was the snappy theme tune. not that it could compare with the original score of the thunderbirds. thankfully early saturday mornings one television channel still plays reruns of the series gerry anderson and his wife created. jonatha frakes should hand in his directors chair, his version was completely hopeless. a waste of film. utter rubbish. a cgi remake may be acceptable but replacing marionettes with homo sapiens subsp. sapiens was a huge error of judgment.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy model \n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "# convert each document to a list of tokens\n",
    "docs = []\n",
    "for doc in nlp.pipe(X_train):\n",
    "    doc_tokens = [token.text for token in doc]\n",
    "    docs.append(doc_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Doc2Vec Model \n",
    "The `Doc2Vec` instances take 2 inputs. A single document that is represented as a list of unicode strings (tokens) and a unique `tag` for the document. Can just be an integer index. \n",
    "\n",
    "The data structure input into `Doc2Vec` should be a list of `TaggedDocument`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag the documents \n",
    "tagged_docs = [TaggedDocument(words= doc, tags=[tag]) for tag, doc in enumerate(docs)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of processing cores \n",
    "cores = multiprocessing.cpu_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model params\n",
    "max_epochs = 100\n",
    "vec_size = 100\n",
    "min_count=2\n",
    "alpha = 0.025\n",
    "dm=1\n",
    "window=10\n",
    "\n",
    "# initialize the model \n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "               min_count=min_count,\n",
    "               dm=dm,\n",
    "               epochs=max_epochs,\n",
    "               window=window, \n",
    "               workers=cores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.build_vocab` builds a dictionary for the model. It consists of all the unique words from the training corpus along with their word count frequency in the corpus. \n",
    "\n",
    "The vocabulary can be access by: `model.wv.vocab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word love is used 10403 times throughout the corpus.\n"
     ]
    }
   ],
   "source": [
    "# get the number of times \"love\" is used in the corpus\n",
    "print(\"Word love is used {} times throughout the corpus.\".format(model.wv.vocab['love'].count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocobulary \n",
    "model.build_vocab(tagged_docs)\n",
    "\n",
    "model.train(documents=tagged_docs, \n",
    "            total_examples=model.corpus_count, \n",
    "            epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model \n",
    "model.save(\"results/d2v.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved model \n",
    "model = Doc2Vec.load(\"results/d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy model \n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "\n",
    "# infer doc vector for a new document \n",
    "test_data = nlp(\"i love the imdb.\")\n",
    "\n",
    "# get the text from the document\n",
    "test_data_text = [token.text for token in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'the', 'imdb', '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the test_data\n",
    "test_data_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer a vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.3734988 ,  0.18976772, -0.6103211 , -0.14179629,  0.0648508 ,\n",
       "        0.74232054, -0.3678494 , -0.45160022, -0.43654352,  0.34054545,\n",
       "        0.09597275,  0.5183157 ,  0.6617551 , -0.03661314,  0.9086173 ,\n",
       "        0.0165403 , -0.930917  , -0.24195996,  0.8548493 ,  0.170209  ,\n",
       "        0.48797396, -0.13557488, -0.690034  , -0.29931948,  0.94054925,\n",
       "        0.2746831 ,  1.129763  , -0.6760563 , -0.103834  , -0.6569413 ,\n",
       "       -0.5803231 , -0.078127  ,  0.35254028, -0.27329898,  0.7407973 ,\n",
       "        0.12073723,  0.06908166, -0.39688852, -0.90500194,  1.1494603 ,\n",
       "       -0.14802633, -0.2030067 ,  0.74796444, -0.9517787 , -0.14173594,\n",
       "       -0.00737411, -0.1972842 ,  0.3129382 , -0.29342565, -1.3055211 ,\n",
       "       -0.12911199,  0.09247501,  0.62931174,  1.1020342 , -0.14113016,\n",
       "        0.13749035, -0.13245723,  0.51655716,  0.10914865,  0.01775738,\n",
       "        0.5450587 ,  0.73628545, -0.18425721,  0.36158788, -0.1964601 ,\n",
       "        0.41417643,  0.30609086, -0.02259944, -0.32677224, -0.92308354,\n",
       "        0.6548052 , -0.14891669,  0.03884804, -0.5335757 , -0.26706296,\n",
       "        0.93302786, -0.00960076, -0.5281856 , -0.35513207, -0.8230319 ,\n",
       "       -0.34422922,  0.56245726,  0.36586484, -0.8234058 , -0.37103817,\n",
       "        0.2611636 , -0.20018996, -0.03789988,  0.9621067 ,  0.16852048,\n",
       "        0.5929444 , -0.88255847,  0.06442527, -0.61015856, -0.18494746,\n",
       "       -0.23729843, -0.5237008 , -0.26012146, -0.39322293,  0.12078387],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the inferred vector for the test document \n",
    "test_data_vector = model.infer_vector(test_data_text)\n",
    "test_data_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that the vector is 100 dimensions \n",
    "len(test_data_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get doc vector for a document in training data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0750184 , -1.9905442 , -4.4494996 , -0.53621876,  3.4369726 ,\n",
       "        2.8636072 ,  1.4251504 , -0.7873193 ,  1.6152562 ,  1.7793663 ,\n",
       "        3.8462756 , -2.4988034 , -0.6999853 ,  3.5884042 ,  1.1586093 ,\n",
       "       -0.8732167 ,  1.7217646 , -0.17224672,  0.46198705,  0.7636378 ,\n",
       "       -0.32666126, -3.6621585 , -3.4452653 ,  0.7645475 , -0.85489446,\n",
       "       -0.67878807,  3.566059  , -2.6591861 , -3.8239892 , -1.3997021 ,\n",
       "        2.7022707 ,  0.4104823 ,  0.5845527 ,  1.5927402 , -2.1032183 ,\n",
       "        0.84438735,  3.537056  , -0.9749475 ,  1.007521  , -1.9738562 ,\n",
       "        1.1589872 ,  1.7555975 , -0.5638223 , -1.5018481 ,  2.7231636 ,\n",
       "       -2.4119732 , -1.0664274 ,  1.5496196 ,  3.2364523 ,  0.6432062 ,\n",
       "        0.45458597,  1.8402778 , -0.77025706,  1.7505522 , -0.06603717,\n",
       "        0.03137324,  0.52317595,  1.290314  ,  0.7653793 , -0.8724251 ,\n",
       "       -0.8246546 ,  1.0141776 ,  0.9919102 , -1.3215085 ,  2.8690574 ,\n",
       "       -0.07864825,  0.25158533,  0.14985803,  1.3841814 , -1.178955  ,\n",
       "        5.062163  , -2.1390107 ,  0.7029668 , -3.610103  ,  0.55227846,\n",
       "       -2.8655925 ,  1.5869362 , -3.1837573 ,  0.38401803,  3.8657851 ,\n",
       "        1.1724001 ,  1.5488461 ,  2.4796476 , -0.5361145 ,  1.3883599 ,\n",
       "        3.6427531 ,  0.3924392 , -0.29289377, -1.1353644 ,  0.06017191,\n",
       "        2.4407904 , -0.5490726 ,  1.8914315 , -2.1730118 , -1.7668576 ,\n",
       "       -1.9689852 , -0.86887825, -0.5008239 ,  0.48550603, -2.2243147 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get doc vector for document with tag 0\n",
    "model.docvecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.7390024 ,  2.581647  , -4.2145786 , -2.2802477 , -0.3221571 ,\n",
       "        4.741527  ,  0.17803015,  0.6950085 ,  0.13797103,  1.9404503 ,\n",
       "        1.6041267 , -1.5517176 , -3.6905885 , -2.1935484 , -0.94091237,\n",
       "       -0.00773263,  0.9710394 ,  0.62761796, -1.1550109 ,  0.18732233,\n",
       "        0.07355768, -3.2035456 ,  0.8499388 ,  0.06021781,  0.8296846 ,\n",
       "        3.2018712 , -1.2890564 ,  0.14946471,  4.286643  , -4.837212  ,\n",
       "        2.058952  , -1.7170444 ,  2.4506824 ,  0.67044514, -0.9461834 ,\n",
       "       -1.9214734 , -0.5804667 , -0.49450612,  0.26986015,  0.09560651,\n",
       "       -3.2881393 ,  1.0870681 ,  3.6450682 ,  0.61791784,  2.1359613 ,\n",
       "       -4.0119777 ,  0.6193669 , -0.42527515,  0.8767335 ,  0.06208834,\n",
       "       -0.51736164,  4.9553204 ,  0.24020366, -0.29787928,  0.8699073 ,\n",
       "        1.5285801 , -2.281047  ,  1.1915747 , -2.7606277 , -0.7646156 ,\n",
       "        5.294057  ,  2.4270234 ,  4.6306705 ,  1.6088039 ,  1.81563   ,\n",
       "       -0.19997375,  2.6723425 ,  0.26849866, -0.90769136,  5.369321  ,\n",
       "        0.72627324,  0.44638035,  3.3789113 , -2.868737  ,  1.5733635 ,\n",
       "       -0.937228  , -1.1742253 , -1.3585402 , -3.3807788 ,  7.694307  ,\n",
       "        0.10012794,  2.373402  , -2.201888  ,  0.05831777, -0.98822194,\n",
       "       -0.06860456, -0.3642996 ,  3.979147  ,  3.1203384 ,  1.2470447 ,\n",
       "        0.6291372 , -1.0181178 , -1.6495355 ,  0.1713701 ,  2.8188329 ,\n",
       "       -1.6172868 , -2.9949894 , -1.4854918 ,  1.0180935 , -0.67033166],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the doc vector for the document with tag 1\n",
    "model.docvecs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most similar document \n",
    "This to returns the document tags along with the cosine similarity score to `doc 1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25471, 0.524448812007904),\n",
       " (4513, 0.46837306022644043),\n",
       " (12130, 0.46624577045440674),\n",
       " (1437, 0.465658038854599),\n",
       " (17813, 0.4653197228908539),\n",
       " (29350, 0.456831693649292),\n",
       " (16058, 0.4560166597366333),\n",
       " (32890, 0.45247572660446167),\n",
       " (33254, 0.45109111070632935),\n",
       " (34499, 0.4502790570259094)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_sim_docs = model.docvecs.most_similar(1)\n",
    "most_sim_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"when i put this movie in my dvd player , and sat down with a coke and some chips , i had some expectations . i was hoping that this movie would contain some of the strong - points of the first movie : awsome animation , good flowing story , excellent voice cast , funny comedy and a kick - ass soundtrack . but , to my disappointment , not any of this is to be found in atlantis : milo 's return . had i read some reviews first , i might not have been so let down . the following paragraph will be directed to those who have seen the first movie , and who enjoyed it primarily for the points mentioned.when the first scene appears , your in for a shock if you just picked atlantis : milo 's return from the display - case at your local videoshop ( or whatever ) , and had the expectations i had . the music feels as a bad imitation of the first movie , and the voice cast has been replaced by a not so fitting one . ( with the exception of a few characters , like the voice of sweet ) . the actual drawings is nt that bad , but the animation in particular is a sad sight . the storyline is also pretty weak , as its more like three episodes of schooby - doo than the single adventurous story we got the last time . but do nt misunderstand , it 's not very good schooby - doo episodes . i did nt laugh a single time , although i might have sniggered once or twice.to the audience who have n't seen the first movie , or do n't especially care for a similar sequel , here is a fast review of this movie as a stand - alone product : if you liked schooby - doo , you might like this movie . if you did n't , you could still enjoy this movie if you have nothing else to do . and i suspect it might be a good kids movie , but i would n't know . it might have been better if milo 's return had been a three - episode series on a cartoon channel , or on breakfast tv .\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at doc 1 \n",
    "query_text = \" \".join(tagged_docs[1].words)\n",
    "query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a dictionary to hold most similar texts \n",
    "# keyed by their index in the TaggedDocuments lists \n",
    "most_similar_texts = {}\n",
    "\n",
    "# get the texts of the most similar docs\n",
    "for most_sim_doc in most_sim_docs:\n",
    "    # get the tagged doc index \n",
    "    index = most_sim_doc[0]\n",
    "    \n",
    "    # convert the tokens from most similar into text\n",
    "    most_sim_text = \" \".join(tagged_docs[index].words)\n",
    "    \n",
    "    # append the text to the list \n",
    "    most_similar_texts[index] = most_sim_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>most_similar_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25471</th>\n",
       "      <td>all dogs go to heaven was a quirky , funny mov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4513</th>\n",
       "      <td>this is a very cool movie . the ending of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12130</th>\n",
       "      <td>this third pokemon movie is too abstract for y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>i wish i could find some good things to say ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17813</th>\n",
       "      <td>it started out slow after an excellent animate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29350</th>\n",
       "      <td>this movie is quite better than the first one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16058</th>\n",
       "      <td>102 dalmatians [ walt disney ] : i was n't a f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32890</th>\n",
       "      <td>i had absolutely nothing to do the past weeken...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33254</th>\n",
       "      <td>i could n't keep from commenting after reading...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34499</th>\n",
       "      <td>they did it again : ripped off an old show 's ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      most_similar_texts\n",
       "25471  all dogs go to heaven was a quirky , funny mov...\n",
       "4513   this is a very cool movie . the ending of the ...\n",
       "12130  this third pokemon movie is too abstract for y...\n",
       "1437   i wish i could find some good things to say ab...\n",
       "17813  it started out slow after an excellent animate...\n",
       "29350  this movie is quite better than the first one ...\n",
       "16058  102 dalmatians [ walt disney ] : i was n't a f...\n",
       "32890  i had absolutely nothing to do the past weeken...\n",
       "33254  i could n't keep from commenting after reading...\n",
       "34499  they did it again : ripped off an old show 's ..."
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the most similar texts into a dataframe \n",
    "most_similar_df = pd.DataFrame(most_similar_texts, index=[1]).transpose().rename(columns={1:'most_similar_texts'})\n",
    "most_similar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"all dogs go to heaven was a quirky , funny movie ; with good name talent who 's voices lended an adult familiarity to a cartoon basicly for kids . it was just interesting enough to be likeable by adults aside from something good for the kids to watch.unfortunately adgth2 is a valueless sequel trying to make a bit of cash rideing on the coattails of the first . charlie sheen is a passable replacement for burt reynolds in this second movie and sheena easton 's voice in a few of the movies lovely but forgettable songs makes her a worthwhile pick as a co - star for this . add dom deluise from the first movie and you 'd think this would be a decent mix to make this sequel at least relatively decent compared to the first one.unfortunately even with the addition of other good voice actors such as bebe neuwirth in the horrible role of anabelle , this movie can not be saved from the atrocious production values and animation skills ( or lack thereof ) present all over this movie . horrible editing , syncronization of the voices , and flat out spaces where characters mouths should be moving to dialouge but are not combine to make this movie look like a college interns animation project instead of the decent sequel it could have been.all in all i 'd say unless you were a very big fan of the first movie i 'd give this a very large pass .\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most similar text 25471\n",
    "most_similar_df.loc[25471][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this is a very cool movie . the ending of the movie is a bit more defined than the play 's ending , but either way it is still a good movie .\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_df.loc[4513][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this third pokemon movie is too abstract for younger kids to follow and too repetitious to entertain older kids . the message of the film-- about dealing with loss-- is subverted by the return of the young girl 's father during the film 's credits . team rocket provide some amusement , but they 're not really part of the small plot , so they do n't appear very often .\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_df.loc[12130][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hate', 0.6397315859794617),\n",
       " ('adore', 0.5936261415481567),\n",
       " ('enjoy', 0.5934271812438965),\n",
       " ('loved', 0.5764082670211792),\n",
       " ('dislike', 0.5725849270820618),\n",
       " ('prefer', 0.535727858543396),\n",
       " ('agree', 0.5338077545166016),\n",
       " ('mean', 0.5323845744132996),\n",
       " ('recommend', 0.5302844047546387),\n",
       " ('think', 0.5271144509315491)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brutal', 0.5958952903747559),\n",
       " ('graphic', 0.593812882900238),\n",
       " ('disturbing', 0.5733826756477356),\n",
       " ('gory', 0.5440038442611694),\n",
       " ('tame', 0.5431349277496338),\n",
       " ('explicit', 0.5158456563949585),\n",
       " ('violence', 0.4995356798171997),\n",
       " ('graphically', 0.48809197545051575),\n",
       " ('nasty', 0.47407007217407227),\n",
       " ('gruesome', 0.4647546410560608)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('violent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stairways', 0.41978752613067627),\n",
       " ('fluorescent', 0.41299641132354736),\n",
       " ('shaven', 0.39963239431381226),\n",
       " ('movement', 0.3951754570007324),\n",
       " ('foam', 0.3854529559612274),\n",
       " ('compadres', 0.38530921936035156),\n",
       " ('pond', 0.3831974267959595),\n",
       " ('cadwell', 0.37259089946746826),\n",
       " ('jello', 0.3722376823425293),\n",
       " ('fur', 0.3714328408241272)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('grass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 0.5081092119216919),\n",
       " ('rabbit', 0.5076563954353333),\n",
       " ('madman', 0.4489584267139435),\n",
       " ('snake', 0.4232267737388611),\n",
       " ('grandpa', 0.4164738655090332),\n",
       " ('mickey', 0.4138184189796448),\n",
       " ('walrus', 0.40957218408584595),\n",
       " ('baby', 0.40911436080932617),\n",
       " ('\"randy', 0.4075496196746826),\n",
       " ('pet', 0.40383607149124146)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('heroes', 0.6354663968086243),\n",
       " ('protagonist', 0.6213669180870056),\n",
       " ('heroine', 0.5825915932655334),\n",
       " ('villain', 0.5639052987098694),\n",
       " ('girlfriend', 0.46861889958381653),\n",
       " ('boyfriend', 0.45519381761550903),\n",
       " ('catchphrase', 0.4545667767524719),\n",
       " ('antagonist', 0.4537510573863983),\n",
       " ('dad', 0.45145243406295776),\n",
       " ('henchman', 0.4488012194633484)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('hero')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess the Model \n",
    "\n",
    "The first sanity check in assessing the model is to compare the document vector obtained through model training to the doc vector produced via the `infer_vector` method of the doc2vec model (use the trainted model to infer the training document vectors). These inferred vectors are expected to be very close to the vectors learned during training and this check just makes sure nothing has gone very wrong with the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
