{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-processing\n",
    "\n",
    "**Tasks**\n",
    "- remove html tags "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# text pre-processing\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# text modelling \n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import spacy\n",
    "\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\" \n",
    "    Produces the text with html tags removed and converts to all lower case. \n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    text (pandas.core.series.Series) A series of text documents. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.core.series.Series A series of text with html tags removed & lower case letters. \n",
    "        \n",
    "    \"\"\"\n",
    "    # initialize a list for cleaned text \n",
    "    clean_text = []\n",
    "    for doc in text:\n",
    "        \n",
    "        ## remove html tags with beautifulsoup \n",
    "        soup = BeautifulSoup(doc)\n",
    "        text = soup.get_text().lower()\n",
    "\n",
    "        # append the text to a new series \n",
    "        clean_text.append(text)\n",
    "\n",
    "    # convert list to a pandas series \n",
    "    clean_text = pd.Series(clean_text)\n",
    "    \n",
    "    return clean_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a dict to store the doc vectors \n",
    "vector_dict = {}\n",
    "colnames = []\n",
    "for i in range(len(model.docvecs)):\n",
    "    # build the dict of doc vectors \n",
    "    vector_dict[i] = model.docvecs[i]\n",
    "    \n",
    "# create the column names\n",
    "for dim in range(vec_size):\n",
    "    colname = \"dim_{0}\".format(dim)\n",
    "    colnames.append(colname)\n",
    "    \n",
    "# create a dataframe of doc vectors\n",
    "vector_df = pd.DataFrame(vector_dict).transpose()\n",
    "# set the col names to be number of dimensions\n",
    "vector_df.columns = colnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/Train.csv\")\n",
    "X_train = train.text\n",
    "y_train = train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I grew up (b. 1965) watching and loving the Th...\n",
       "1    When I put this movie in my DVD player, and sa...\n",
       "2    Why do people who do not know what a particula...\n",
       "3    Even though I have great interest in Biblical ...\n",
       "4    Im a die hard Dads Army fan and nothing will e...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40000 training documents.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} training documents.\".format(len(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-process text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process the text \n",
    "X_train = preprocess_text(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i grew up (b. 1965) watching and loving the thunderbirds. all my mates at school watched. we played \"thunderbirds\" before school, during lunch and after school. we all wanted to be virgil or scott. no one wanted to be alan. counting down from 5 became an art form. i took my children to see the movie hoping they would get a glimpse of what i loved as a child. how bitterly disappointing. the only high point was the snappy theme tune. not that it could compare with the original score of the thunderbirds. thankfully early saturday mornings one television channel still plays reruns of the series gerry anderson and his wife created. jonatha frakes should hand in his directors chair, his version was completely hopeless. a waste of film. utter rubbish. a cgi remake may be acceptable but replacing marionettes with homo sapiens subsp. sapiens was a huge error of judgment.'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i grew up (b. 1965) watching and loving the th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when i put this movie in my dvd player, and sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why do people who do not know what a particula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>even though i have great interest in biblical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im a die hard dads army fan and nothing will e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  i grew up (b. 1965) watching and loving the th...\n",
       "1  when i put this movie in my dvd player, and sa...\n",
       "2  why do people who do not know what a particula...\n",
       "3  even though i have great interest in biblical ...\n",
       "4  im a die hard dads army fan and nothing will e..."
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_clean_df = X_train.to_frame()\n",
    "X_train_clean_df.columns = [\"text\"]\n",
    "X_train_clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned train data \n",
    "X_train_clean_df.to_csv(\"data/X_train_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy model \n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "# convert each document to a list of tokens\n",
    "docs = []\n",
    "for doc in nlp.pipe(X_train):\n",
    "    doc_tokens = [token.text for token in doc]\n",
    "    docs.append(doc_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Doc2Vec Model \n",
    "The `Doc2Vec` instances take 2 inputs. A single document that is represented as a list of unicode strings (tokens) and a unique `tag` for the document. Can just be an integer index. \n",
    "\n",
    "The data structure input into `Doc2Vec` should be a list of `TaggedDocument`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag the documents \n",
    "tagged_docs = [TaggedDocument(words= doc, tags=[tag]) for tag, doc in enumerate(docs)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of processing cores \n",
    "cores = multiprocessing.cpu_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model params\n",
    "max_epochs = 100\n",
    "vec_size = 100\n",
    "min_count=2\n",
    "alpha = 0.025\n",
    "dm=1\n",
    "window=10\n",
    "\n",
    "# initialize the model \n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "               min_count=min_count,\n",
    "               dm=dm,\n",
    "               epochs=max_epochs,\n",
    "               window=window, \n",
    "               workers=cores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.build_vocab` builds a dictionary for the model. It consists of all the unique words from the training corpus along with their word count frequency in the corpus. \n",
    "\n",
    "The vocabulary can be access by: `model.wv.vocab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocobulary \n",
    "model.build_vocab(tagged_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word love is used 10403 times throughout the corpus.\n"
     ]
    }
   ],
   "source": [
    "# get the number of times \"love\" is used in the corpus\n",
    "print(\"Word love is used {} times throughout the corpus.\".format(model.wv.vocab['love'].count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.train(documents=tagged_docs, \n",
    "            total_examples=model.corpus_count, \n",
    "            epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model \n",
    "model.save(\"results/d2v.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved model \n",
    "model = Doc2Vec.load(\"results/d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy model \n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "\n",
    "# infer doc vector for a new document \n",
    "test_data = nlp(\"i love the imdb.\")\n",
    "\n",
    "# get the text from the document\n",
    "test_data_text = [token.text for token in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'the', 'imdb', '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the test_data\n",
    "test_data_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer a vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3660542 , -0.35544372, -0.16209263, -0.31187433, -0.3878265 ,\n",
       "       -0.39781448,  0.86542064,  0.39633673, -0.6795637 , -0.6038303 ,\n",
       "       -0.3122509 ,  0.4093655 ,  0.12963279,  0.1514839 , -0.04725248,\n",
       "        0.04435014, -0.30326045,  0.55323535, -0.24522036, -0.6166701 ,\n",
       "       -0.16535714, -0.6879205 ,  0.53495556,  0.6721085 , -0.26523936,\n",
       "       -0.25310084,  0.6211377 ,  0.57464284, -0.27010357,  0.14269952,\n",
       "       -0.49309996, -0.11419404,  1.4739265 ,  0.24999467, -0.38384798,\n",
       "        0.0229602 ,  0.6399379 ,  1.1553128 , -0.6511551 , -0.31271848,\n",
       "        0.0299325 , -0.08276461, -0.15362152,  0.21290593,  0.33925855,\n",
       "        0.22131093,  0.12085311,  0.58260417, -0.3449641 ,  0.4908272 ,\n",
       "       -0.6026803 ,  0.21922089,  0.01464912, -0.83664036, -0.48538578,\n",
       "        0.2655007 ,  0.571258  , -0.4635811 , -1.1522716 , -1.2571613 ,\n",
       "       -0.91198575, -0.1254218 ,  0.07205608, -0.2206613 , -0.78305876,\n",
       "        0.02976534,  0.12442748, -0.6776802 ,  0.58454186, -0.21756212,\n",
       "        0.03757276,  0.10979874,  0.4027019 ,  0.21908762, -0.8071368 ,\n",
       "       -0.10933816, -0.4765943 , -0.21240401,  0.6080637 , -0.03676366,\n",
       "       -0.3226256 , -0.41886413, -0.03263313,  0.27763745,  0.6600899 ,\n",
       "       -0.711778  ,  0.21178824, -0.8716206 ,  0.94735956,  0.47207665,\n",
       "        0.11668964,  0.45783597, -0.03710304,  0.4638841 ,  0.166569  ,\n",
       "       -0.34149665,  0.49974975,  0.59477556, -0.6136176 , -0.02057221],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the inferred vector for the test document \n",
    "test_data_vector = model.infer_vector(test_data_text)\n",
    "test_data_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that the vector is 100 dimensions \n",
    "len(test_data_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get doc vector for a document in training data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.78455496, -0.3975019 , -0.33198473,  0.96559227,  2.0129318 ,\n",
       "        1.8874876 ,  0.5107813 ,  3.2886736 ,  3.4330976 , -3.7912304 ,\n",
       "       -0.39993343,  1.5135373 ,  0.43739882,  0.38999793, -1.2009306 ,\n",
       "       -1.7992256 ,  1.1126815 ,  3.2383354 , -0.6476589 , -0.9192871 ,\n",
       "        1.3353504 ,  0.20898631, -2.086641  ,  4.331556  ,  1.078577  ,\n",
       "        3.1066291 ,  1.0192515 , -0.995883  , -0.25814018, -3.7610812 ,\n",
       "       -1.5125705 , -3.084904  , -2.0255165 , -1.2520654 ,  2.9139643 ,\n",
       "       -0.35589314,  2.7762187 ,  1.2263558 ,  0.9764989 ,  1.4696493 ,\n",
       "        1.4430372 , -1.3313451 ,  1.0044692 ,  1.668386  ,  0.0148002 ,\n",
       "        1.4403996 ,  0.5381073 , -1.1963979 , -3.035492  , -1.0050448 ,\n",
       "       -0.22705024, -1.8456436 , -0.25627303,  1.8898085 , -1.9619877 ,\n",
       "       -1.9410548 , -1.2830682 ,  0.47285038, -0.6256406 ,  2.8846972 ,\n",
       "       -3.8962908 , -1.5789651 ,  0.6843119 ,  1.1497818 , -0.8217583 ,\n",
       "        0.9814244 , -0.49725807,  2.9933703 ,  1.2505748 ,  0.27889073,\n",
       "       -0.4924347 , -2.217509  ,  0.31758472,  2.15224   , -0.75350016,\n",
       "       -0.5054921 , -3.5577893 ,  0.7601551 , -0.28576377, -1.7238655 ,\n",
       "       -0.11268797, -0.2007938 , -2.9459436 ,  1.570772  ,  3.8871047 ,\n",
       "       -0.0907405 ,  0.01301496, -1.5513376 , -0.36384308,  1.8073013 ,\n",
       "       -1.2440865 ,  1.1928841 ,  0.89124197,  2.8665967 ,  8.549128  ,\n",
       "       -0.47877753,  1.6920872 ,  2.4171042 ,  1.7719048 , -1.3122506 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get doc vector for document with tag 0\n",
    "model.docvecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0276638 , -3.7865596 , -1.5838888 ,  3.1244397 , -0.8921881 ,\n",
       "       -1.5585665 ,  3.3190892 ,  1.9880208 ,  2.08938   , -0.5240976 ,\n",
       "       -2.2305508 ,  3.5747876 , -1.9769565 ,  1.8865669 ,  0.38790658,\n",
       "        3.4950302 , -2.8666272 ,  5.3179765 ,  2.8796036 , -0.12509392,\n",
       "       -4.834104  , -0.20956892, -1.3277993 ,  0.6330954 , -1.9985896 ,\n",
       "        3.5359511 ,  0.0689284 , -2.369902  , -0.10371426,  1.5585757 ,\n",
       "       -1.0368974 ,  1.5165955 ,  2.4063566 , -0.2352102 ,  3.966048  ,\n",
       "        1.9501489 ,  5.435891  ,  1.4581848 ,  1.5568197 , -1.9583993 ,\n",
       "        1.6910241 ,  0.55866873,  1.5060309 , -2.7832923 ,  0.11567657,\n",
       "        3.2735307 ,  0.39607665, -1.6610876 , -1.4223877 , -1.536292  ,\n",
       "       -0.59399366, -0.19992135,  0.06863879, -1.8700184 ,  0.01535263,\n",
       "        0.95391506, -1.1874477 ,  0.05783149,  0.427828  , -0.26444364,\n",
       "       -4.614383  , -0.66521543,  1.0540657 , -1.4417485 , -3.6814992 ,\n",
       "        4.929885  , -0.8851586 ,  1.9309868 , -2.2361135 ,  0.9083246 ,\n",
       "        1.8603902 , -1.974297  , -2.0800133 , -0.7201579 ,  2.2433846 ,\n",
       "        1.6472554 , -1.1895399 , -0.62853116,  1.8228191 , -2.3522367 ,\n",
       "       -5.8947597 ,  1.2262658 , -0.03442017, -1.8491086 , -1.0716869 ,\n",
       "        2.8467553 ,  3.3181167 ,  3.0999296 , -0.23901515, -1.1340805 ,\n",
       "        0.39758644,  2.5952525 ,  2.952577  , -2.6841803 ,  1.7536578 ,\n",
       "       -1.2006419 , -0.7143993 , -1.4403499 ,  1.5491996 , -1.6139704 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the doc vector for the document with tag 1\n",
    "model.docvecs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most similar document \n",
    "This to returns the document tags along with the cosine similarity score to `doc 1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(21946, 0.47781458497047424),\n",
       " (25471, 0.4739196300506592),\n",
       " (33118, 0.45629557967185974),\n",
       " (34488, 0.4521036744117737),\n",
       " (17886, 0.45017537474632263),\n",
       " (4513, 0.44906243681907654),\n",
       " (25708, 0.44853517413139343),\n",
       " (26544, 0.4482578933238983),\n",
       " (18306, 0.44417983293533325),\n",
       " (34499, 0.4436340034008026)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_sim_docs = model.docvecs.most_similar(1)\n",
    "most_sim_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"when i put this movie in my dvd player , and sat down with a coke and some chips , i had some expectations . i was hoping that this movie would contain some of the strong - points of the first movie : awsome animation , good flowing story , excellent voice cast , funny comedy and a kick - ass soundtrack . but , to my disappointment , not any of this is to be found in atlantis : milo 's return . had i read some reviews first , i might not have been so let down . the following paragraph will be directed to those who have seen the first movie , and who enjoyed it primarily for the points mentioned.when the first scene appears , your in for a shock if you just picked atlantis : milo 's return from the display - case at your local videoshop ( or whatever ) , and had the expectations i had . the music feels as a bad imitation of the first movie , and the voice cast has been replaced by a not so fitting one . ( with the exception of a few characters , like the voice of sweet ) . the actual drawings is nt that bad , but the animation in particular is a sad sight . the storyline is also pretty weak , as its more like three episodes of schooby - doo than the single adventurous story we got the last time . but do nt misunderstand , it 's not very good schooby - doo episodes . i did nt laugh a single time , although i might have sniggered once or twice.to the audience who have n't seen the first movie , or do n't especially care for a similar sequel , here is a fast review of this movie as a stand - alone product : if you liked schooby - doo , you might like this movie . if you did n't , you could still enjoy this movie if you have nothing else to do . and i suspect it might be a good kids movie , but i would n't know . it might have been better if milo 's return had been a three - episode series on a cartoon channel , or on breakfast tv .\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at doc 1 \n",
    "query_text = \" \".join(tagged_docs[1].words)\n",
    "query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a dictionary to hold most similar texts \n",
    "# keyed by their index in the TaggedDocuments lists \n",
    "most_similar_texts = {}\n",
    "\n",
    "# get the texts of the most similar docs\n",
    "for most_sim_doc in most_sim_docs:\n",
    "    # get the tagged doc index \n",
    "    index = most_sim_doc[0]\n",
    "    \n",
    "    # convert the tokens from most similar into text\n",
    "    most_sim_text = \" \".join(tagged_docs[index].words)\n",
    "    \n",
    "    # append the text to the list \n",
    "    most_similar_texts[index] = most_sim_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>most_similar_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21946</th>\n",
       "      <td>very bad movie ........ and i mean very bad .....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25471</th>\n",
       "      <td>all dogs go to heaven was a quirky , funny mov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33118</th>\n",
       "      <td>since watching the trailer in \" the little mer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34488</th>\n",
       "      <td>to be clear from the get go , ' the bagman ' i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17886</th>\n",
       "      <td>this movie is just plain silly . almost every ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4513</th>\n",
       "      <td>this is a very cool movie . the ending of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25708</th>\n",
       "      <td>i had a bit of hope for this hour long film ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26544</th>\n",
       "      <td>i did n't enjoy this film . i thought the acti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18306</th>\n",
       "      <td>my main criticism with the movie is the animat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34499</th>\n",
       "      <td>they did it again : ripped off an old show 's ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      most_similar_texts\n",
       "21946  very bad movie ........ and i mean very bad .....\n",
       "25471  all dogs go to heaven was a quirky , funny mov...\n",
       "33118  since watching the trailer in \" the little mer...\n",
       "34488  to be clear from the get go , ' the bagman ' i...\n",
       "17886  this movie is just plain silly . almost every ...\n",
       "4513   this is a very cool movie . the ending of the ...\n",
       "25708  i had a bit of hope for this hour long film ma...\n",
       "26544  i did n't enjoy this film . i thought the acti...\n",
       "18306  my main criticism with the movie is the animat...\n",
       "34499  they did it again : ripped off an old show 's ..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the most similar texts into a dataframe \n",
    "most_similar_df = pd.DataFrame(most_similar_texts, index=[1]).transpose().rename(columns={1:'most_similar_texts'})\n",
    "most_similar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"very bad movie ........ and i mean very bad ... the plot is predictable , and it 's eally cheesy , the creativeness of the battle and the dance scenes for the time are the only reason i did n't give the movie a one , other than that ... this is def a movie one can def afford not to watch ..... i feel while watching the movie , the idea behind the movie was an interesting one tho kind of cliché .... bringing country bumpkins to the city blah blah blah , but i feel it might have been at least a little better if it just was n't so cheesy , very poorly portrayed from idea to screen , i think . the plot is somewhat predictable at times , tho the dancing i can say at times , is pretty good , the break dance battle twist was good ..... if u just pop the movie and watch the dance scenes and make up your own dialog maybe it can be a 5 ... lol\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most similar text 25471\n",
    "most_similar_df.loc[21946][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"all dogs go to heaven was a quirky , funny movie ; with good name talent who 's voices lended an adult familiarity to a cartoon basicly for kids . it was just interesting enough to be likeable by adults aside from something good for the kids to watch.unfortunately adgth2 is a valueless sequel trying to make a bit of cash rideing on the coattails of the first . charlie sheen is a passable replacement for burt reynolds in this second movie and sheena easton 's voice in a few of the movies lovely but forgettable songs makes her a worthwhile pick as a co - star for this . add dom deluise from the first movie and you 'd think this would be a decent mix to make this sequel at least relatively decent compared to the first one.unfortunately even with the addition of other good voice actors such as bebe neuwirth in the horrible role of anabelle , this movie can not be saved from the atrocious production values and animation skills ( or lack thereof ) present all over this movie . horrible editing , syncronization of the voices , and flat out spaces where characters mouths should be moving to dialouge but are not combine to make this movie look like a college interns animation project instead of the decent sequel it could have been.all in all i 'd say unless you were a very big fan of the first movie i 'd give this a very large pass .\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_df.loc[25471][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'since watching the trailer in \" the little mermaid ii : return to the sea \" dvd , i had a feeling that this movie is gon na be great \\'cause i am a huge disney fan . and guess what ? i \\'m right ! this movie is a very worthy successor to the original classic \" lady and the tramp\".it tells the story of scamp , lady and tramp \\'s mischievious son scamp , who wants to be wild and free instead of living a housedog life . though the movie might not be as good as the first one , it has a great moral that you could n\\'t find anywhere else until you watch it.i admit that the movie is n\\'t for everyone , but those of you who hate it , all i can say is that you do n\\'t have a spirit for this and i suggest that you should n\\'t go see it again . but hey ! it \\'s really an awesome story , packed with brilliant animation , music , and star - studded voice talents featuring scott wolf(party of five ) and alyssa milano(charmed ) . so if you have n\\'t seen the movie , why standing there ? go and grab the copy ! ! !'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_df.loc[33118][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hate', 0.5928248167037964),\n",
       " ('loved', 0.5912954211235046),\n",
       " ('enjoy', 0.5877125263214111),\n",
       " ('mean', 0.5757228136062622),\n",
       " ('adore', 0.5671936273574829),\n",
       " ('recommend', 0.5667845010757446),\n",
       " ('think', 0.5659958124160767),\n",
       " ('suggest', 0.5496004223823547),\n",
       " ('dislike', 0.5439720153808594),\n",
       " ('liked', 0.5359171628952026)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brutal', 0.6238125562667847),\n",
       " ('graphic', 0.5960907936096191),\n",
       " ('tame', 0.5844867825508118),\n",
       " ('disturbing', 0.5794614553451538),\n",
       " ('violence', 0.5529541969299316),\n",
       " ('gory', 0.5460259914398193),\n",
       " ('explicit', 0.5345040559768677),\n",
       " ('nasty', 0.5000576972961426),\n",
       " ('vicious', 0.4735429286956787),\n",
       " ('violence.the', 0.4725140929222107)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('violent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('compadres', 0.4567449688911438),\n",
       " ('airless', 0.4091748595237732),\n",
       " ('stairways', 0.4077175259590149),\n",
       " ('halo', 0.40057915449142456),\n",
       " ('wildebeests', 0.38299745321273804),\n",
       " ('foliage', 0.3772423267364502),\n",
       " ('marebito', 0.3742396831512451),\n",
       " ('champs', 0.37367862462997437),\n",
       " ('jello', 0.3735056519508362),\n",
       " ('fluorescent', 0.3645794987678528)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('grass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 0.5416181087493896),\n",
       " ('rabbit', 0.4458198547363281),\n",
       " ('sloth', 0.437106192111969),\n",
       " ('shark', 0.42957472801208496),\n",
       " ('jack', 0.4259747564792633),\n",
       " ('pet', 0.41923123598098755),\n",
       " ('twin', 0.4170091152191162),\n",
       " ('puppetmaster', 0.4132137894630432),\n",
       " ('snake', 0.411146879196167),\n",
       " ('parrot', 0.40274378657341003)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('heroes', 0.6207660436630249),\n",
       " ('protagonist', 0.6020461320877075),\n",
       " ('heroine', 0.5781691074371338),\n",
       " ('villain', 0.5496834516525269),\n",
       " ('everyman', 0.48411333560943604),\n",
       " ('boyfriend', 0.47233858704566956),\n",
       " ('girlfriend', 0.4598536193370819),\n",
       " ('antagonist', 0.4566743075847626),\n",
       " ('partner', 0.454791784286499),\n",
       " ('superhero', 0.4533049166202545)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('hero')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Doc Vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a dict to store the doc vectors \n",
    "vector_dict = {}\n",
    "colnames = []\n",
    "for i in range(len(model.docvecs)):\n",
    "    # build the dict of doc vectors \n",
    "    vector_dict[i] = model.docvecs[i]\n",
    "    \n",
    "# create the column names\n",
    "for dim in range(vec_size):\n",
    "    colname = \"dim_{0}\".format(dim)\n",
    "    colnames.append(colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of doc vectors\n",
    "vector_df = pd.DataFrame(vector_dict).transpose()\n",
    "# set the col names to be number of dimensions\n",
    "vector_df.columns = colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40000 documents; each document is represented in 100 dimensions.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {0} documents; each document is represented in {1} dimensions.\".format(vector_df.shape[0], vector_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_0</th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "      <th>dim_9</th>\n",
       "      <th>...</th>\n",
       "      <th>dim_90</th>\n",
       "      <th>dim_91</th>\n",
       "      <th>dim_92</th>\n",
       "      <th>dim_93</th>\n",
       "      <th>dim_94</th>\n",
       "      <th>dim_95</th>\n",
       "      <th>dim_96</th>\n",
       "      <th>dim_97</th>\n",
       "      <th>dim_98</th>\n",
       "      <th>dim_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.784555</td>\n",
       "      <td>-0.397502</td>\n",
       "      <td>-0.331985</td>\n",
       "      <td>0.965592</td>\n",
       "      <td>2.012932</td>\n",
       "      <td>1.887488</td>\n",
       "      <td>0.510781</td>\n",
       "      <td>3.288674</td>\n",
       "      <td>3.433098</td>\n",
       "      <td>-3.791230</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.244087</td>\n",
       "      <td>1.192884</td>\n",
       "      <td>0.891242</td>\n",
       "      <td>2.866597</td>\n",
       "      <td>8.549128</td>\n",
       "      <td>-0.478778</td>\n",
       "      <td>1.692087</td>\n",
       "      <td>2.417104</td>\n",
       "      <td>1.771905</td>\n",
       "      <td>-1.312251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.027664</td>\n",
       "      <td>-3.786560</td>\n",
       "      <td>-1.583889</td>\n",
       "      <td>3.124440</td>\n",
       "      <td>-0.892188</td>\n",
       "      <td>-1.558566</td>\n",
       "      <td>3.319089</td>\n",
       "      <td>1.988021</td>\n",
       "      <td>2.089380</td>\n",
       "      <td>-0.524098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397586</td>\n",
       "      <td>2.595253</td>\n",
       "      <td>2.952577</td>\n",
       "      <td>-2.684180</td>\n",
       "      <td>1.753658</td>\n",
       "      <td>-1.200642</td>\n",
       "      <td>-0.714399</td>\n",
       "      <td>-1.440350</td>\n",
       "      <td>1.549200</td>\n",
       "      <td>-1.613970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.591049</td>\n",
       "      <td>-1.170077</td>\n",
       "      <td>0.830686</td>\n",
       "      <td>3.815988</td>\n",
       "      <td>-0.730998</td>\n",
       "      <td>0.646123</td>\n",
       "      <td>4.216710</td>\n",
       "      <td>3.768117</td>\n",
       "      <td>1.581684</td>\n",
       "      <td>0.660360</td>\n",
       "      <td>...</td>\n",
       "      <td>2.918566</td>\n",
       "      <td>0.399827</td>\n",
       "      <td>-1.357417</td>\n",
       "      <td>-0.124357</td>\n",
       "      <td>0.657655</td>\n",
       "      <td>-2.304216</td>\n",
       "      <td>-3.644226</td>\n",
       "      <td>-0.174466</td>\n",
       "      <td>2.901997</td>\n",
       "      <td>0.991823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.579851</td>\n",
       "      <td>0.370804</td>\n",
       "      <td>0.325788</td>\n",
       "      <td>2.724860</td>\n",
       "      <td>0.076738</td>\n",
       "      <td>0.271273</td>\n",
       "      <td>2.576356</td>\n",
       "      <td>1.767729</td>\n",
       "      <td>-1.268051</td>\n",
       "      <td>-3.105602</td>\n",
       "      <td>...</td>\n",
       "      <td>1.757116</td>\n",
       "      <td>-3.508960</td>\n",
       "      <td>-0.034007</td>\n",
       "      <td>0.554777</td>\n",
       "      <td>2.150062</td>\n",
       "      <td>1.295318</td>\n",
       "      <td>-0.126439</td>\n",
       "      <td>2.238556</td>\n",
       "      <td>1.850339</td>\n",
       "      <td>-1.993807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.058910</td>\n",
       "      <td>-1.690887</td>\n",
       "      <td>-0.004952</td>\n",
       "      <td>-2.187342</td>\n",
       "      <td>-3.583359</td>\n",
       "      <td>2.466982</td>\n",
       "      <td>2.651749</td>\n",
       "      <td>0.484369</td>\n",
       "      <td>1.344823</td>\n",
       "      <td>-2.241074</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.418538</td>\n",
       "      <td>-4.106030</td>\n",
       "      <td>2.691415</td>\n",
       "      <td>-2.924291</td>\n",
       "      <td>-0.589641</td>\n",
       "      <td>-2.774781</td>\n",
       "      <td>-1.087569</td>\n",
       "      <td>-1.415747</td>\n",
       "      <td>-0.630574</td>\n",
       "      <td>0.065931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dim_0     dim_1     dim_2     dim_3     dim_4     dim_5     dim_6  \\\n",
       "0  0.784555 -0.397502 -0.331985  0.965592  2.012932  1.887488  0.510781   \n",
       "1  1.027664 -3.786560 -1.583889  3.124440 -0.892188 -1.558566  3.319089   \n",
       "2  0.591049 -1.170077  0.830686  3.815988 -0.730998  0.646123  4.216710   \n",
       "3 -1.579851  0.370804  0.325788  2.724860  0.076738  0.271273  2.576356   \n",
       "4 -1.058910 -1.690887 -0.004952 -2.187342 -3.583359  2.466982  2.651749   \n",
       "\n",
       "      dim_7     dim_8     dim_9  ...    dim_90    dim_91    dim_92    dim_93  \\\n",
       "0  3.288674  3.433098 -3.791230  ... -1.244087  1.192884  0.891242  2.866597   \n",
       "1  1.988021  2.089380 -0.524098  ...  0.397586  2.595253  2.952577 -2.684180   \n",
       "2  3.768117  1.581684  0.660360  ...  2.918566  0.399827 -1.357417 -0.124357   \n",
       "3  1.767729 -1.268051 -3.105602  ...  1.757116 -3.508960 -0.034007  0.554777   \n",
       "4  0.484369  1.344823 -2.241074  ... -2.418538 -4.106030  2.691415 -2.924291   \n",
       "\n",
       "     dim_94    dim_95    dim_96    dim_97    dim_98    dim_99  \n",
       "0  8.549128 -0.478778  1.692087  2.417104  1.771905 -1.312251  \n",
       "1  1.753658 -1.200642 -0.714399 -1.440350  1.549200 -1.613970  \n",
       "2  0.657655 -2.304216 -3.644226 -0.174466  2.901997  0.991823  \n",
       "3  2.150062  1.295318 -0.126439  2.238556  1.850339 -1.993807  \n",
       "4 -0.589641 -2.774781 -1.087569 -1.415747 -0.630574  0.065931  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the first few documents \n",
    "# each row represents a movie review \n",
    "vector_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the document vector dataset\n",
    "vector_df.to_csv(\"data/train_d2v.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
